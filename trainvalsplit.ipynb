{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/eunseo/AUE8088-PA2/datasets/kaist-rgbt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt_path = os.path.join(base_dir,'train-all-04.txt')\n",
    "output_train_txt_path = os.path.join(base_dir,'train_split.txt')\n",
    "output_val_txt_path = os.path.join(base_dir,'val_split.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_txt_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "train_lines, val_lines = train_test_split(lines, test_size=0.2, random_state=10)\n",
    "train_lines = sorted(train_lines)\n",
    "val_lines = sorted(val_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_lines: 10030\n",
      "#val_lines: 2508\n"
     ]
    }
   ],
   "source": [
    "with open(output_train_txt_path, 'w+') as file:\n",
    "    for line in train_lines:\n",
    "        file.write(line)\n",
    "    file.seek(0)\n",
    "    train_cnt = len(file.readlines())\n",
    "print(f'#train_lines: {train_cnt}')\n",
    "\n",
    "with open(output_val_txt_path, 'w+') as file:\n",
    "    for line in val_lines:\n",
    "        file.write(line)\n",
    "    file.seek(0)\n",
    "    val_cnt = len(file.readlines())\n",
    "print(f'#val_lines: {val_cnt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create KAIST_annotation.json for validation subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "json_data = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [\n",
    "            {\"id\": 0, \"name\": \"person\"},\n",
    "            {\"id\": 1, \"name\": \"cyclist\"},\n",
    "            {\"id\": 2, \"name\": \"people\"},\n",
    "            {\"id\": 3, \"name\": \"person?\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "name_to_id = {category[\"name\"]: category[\"id\"] for category in json_data[\"categories\"]}\n",
    "image_id = 0\n",
    "annotation_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_xml_path = os.path.join(base_dir, 'train/labels-xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_val_txt_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        image_path = line.strip()  \n",
    "        image_filename = os.path.basename(image_path)  \n",
    "        xml_filename = image_filename.replace('.jpg', '.xml') \n",
    "        xml_path = os.path.join(labels_xml_path, xml_filename)\n",
    "\n",
    "        print(xml_path, \"/t conversion to json completed\")\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        json_data[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"im_name\": root.find('filename').text,\n",
    "            \"height\": int(root.find('size/height').text),\n",
    "            \"width\": int(root.find('size/width').text)\n",
    "        })\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            bbox = obj.find('bndbox')\n",
    "            x = int(bbox.find('x').text)\n",
    "            y = int(bbox.find('y').text)\n",
    "            w = int(bbox.find('w').text)\n",
    "            h = int(bbox.find('h').text)\n",
    "\n",
    "            json_data[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": int(name_to_id.get(obj.find('name').text)), \n",
    "                \"bbox\": [x, y, w, h],\n",
    "                \"height\": h,\n",
    "                \"occlusion\": int(obj.find('occlusion').text),\n",
    "                \"ignore\": int(obj.find('difficult').text) if obj.find('difficult') is not None else 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "        image_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/eval/KAIST_annotation.json', 'w') as outfile:\n",
    "    json.dump(json_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download script/URL (optional) ---------------------------------------------------------------------------------------\n",
    "download: |\n",
    "  import os\n",
    "  import xml.etree.ElementTree as ET\n",
    "  from tqdm import tqdm\n",
    "  from utils.general import download, Path\n",
    "  from utils.general import Path\n",
    "\n",
    "  def convert_label(names, lb_path, lb_path_new):\n",
    "      def convert_box(size, box):\n",
    "          dw, dh = 1. / size[0], 1. / size[1]\n",
    "          x, y, w, h = box\n",
    "          return x * dw, y * dh, w * dw, h * dh\n",
    "\n",
    "      with open(lb_path) as in_file:\n",
    "          tree = ET.parse(in_file)\n",
    "\n",
    "      with open(lb_path_new, 'w') as out_file:\n",
    "          root = tree.getroot()\n",
    "          size = root.find('size')\n",
    "          w = int(size.find('width').text)\n",
    "          h = int(size.find('height').text)\n",
    "\n",
    "          for obj in root.iter('object'):\n",
    "              cls = obj.find('name').text\n",
    "              xmlbox = obj.find('bndbox')\n",
    "              bb = convert_box((w, h), [float(xmlbox.find(x).text) for x in ('x', 'y', 'w', 'h')])\n",
    "              occ = int(obj.find('occlusion').text)\n",
    "              cls_id = names.index(cls)  # class id\n",
    "              out_file.write(\" \".join([str(a) for a in (cls_id, *bb, occ)]) + '\\n')\n",
    "\n",
    "  path = Path('/home/ubuntu/datasets/kaist-cvpr15-aue8088')  # dataset root dir\n",
    "  with open(str(path / 'train-all-04.txt'), 'r') as fp:\n",
    "  # with open(str(path / 'test-all-20.txt'), 'r') as fp:\n",
    "      labels = [f.replace('{}/', '').replace('images', 'labels').replace('.jpg', '.xml').rstrip('\\n') for f in fp.readlines()]\n",
    "\n",
    "  with open(path / 'kaist-rgbt.names') as fp:\n",
    "      names = [f.rstrip('\\n') for f in fp.readlines()]\n",
    "\n",
    "  for label in tqdm(labels, total=len(labels)):\n",
    "      lb_file = str(path / label)\n",
    "      lb_file_new = lb_file.replace('labels', 'labels_converted').replace('.xml', '.txt')\n",
    "\n",
    "      os.makedirs(os.path.dirname(lb_file_new), exist_ok=True)\n",
    "\n",
    "      convert_label(names, lb_file, lb_file_new)  # convert labels to YOLO format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
